from fastapi import FastAPI, UploadFile, File, Form, Request, BackgroundTasks, Header
from fastapi.responses import FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from bs4 import BeautifulSoup
from typing import List, Dict, Any
import requests

import os
import re
import shutil
import logging


from .config import STATIC_DIR, DOCS_DIR, LOG_FILE, CHROMA_DIR
from .database import (
    get_vectordb,
    index_document,
    get_indexed_files,
    remove_document,
    reset_vectordb
)
from .semantic_search import (
    SearchInput,
    SearchResult,
    get_retrieved_chunks_before_llm
)
from .slack import clean_mention, post_slack_reply
from .summary import summarize_slack_thread
from .slack import handle_slack_message
from .utils import format_sources

from langchain.chains import RetrievalQA
from .qa_utils import run_qa_chain
from slack_sdk.web.async_client import AsyncWebClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(LOG_FILE, encoding="utf-8")
    ]
)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI()
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

@app.get("/admin")
async def root():
    return FileResponse(f"{STATIC_DIR}/index.html")

@app.get("/indexing")
async def root():
    return FileResponse(f"{STATIC_DIR}/text-indexing.html")

@app.on_event("startup")
async def startup_event():
    try:
        logger.info("ğŸš€ Server starting up...")
        indexed_count = 0
        skipped_count = 0

        for filename in os.listdir(DOCS_DIR):
            if filename.endswith((".pdf", ".txt")):
                file_path = os.path.join(DOCS_DIR, filename)
                file_type = "pdf" if filename.endswith(".pdf") else "txt"

                # index_documentì˜ ê²°ê³¼ë¡œ ì„±ê³µ ì—¬ë¶€ ë°˜í™˜
                result = index_document(file_path, file_type)

                if result is True:
                    indexed_count += 1
                elif result == "skipped":
                    skipped_count += 1
                else:
                    logger.warning(f"â“ Unknown result from indexing '{filename}': {result}")

        logger.info(f"ğŸ“š Startup indexing complete: {indexed_count} files indexed, {skipped_count} files unchanged")

    except Exception as e:
        import traceback
        traceback.print_exc()
        logger.error(f"âŒ Error during startup: {str(e)}")

@app.post("/upload_pdf/")
async def upload_pdf(file: UploadFile = File(...)):
    try:
        file_path = os.path.join(DOCS_DIR, file.filename)
        with open(file_path, "wb") as f:
            shutil.copyfileobj(file.file, f)
            
        if index_document(file_path, "pdf"):
            return {"message": f"'{file.filename}' indexed successfully"}
        else:
            return JSONResponse(status_code=500, content={"error": "Failed to index document"})
            
    except Exception as e:
        logger.error(f"âŒ Error uploading PDF: {str(e)}")
        return JSONResponse(status_code=500, content={"error": str(e)})

def extract_questions(text: str) -> set:
    """Q1:, Q2: í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì§ˆë¬¸ ì¶”ì¶œ"""
    return set(
        m.strip()
        for m in re.findall(r"Q\d+:\s*(.+?)(?:\n|$)", text)
    )


@app.post("/upload_text/")
async def upload_text(text: str = Form(...), source: str = Form(...)):
    try:
        # 1. ì…ë ¥ê°’ ê²€ì¦
        if not text.strip():
            return JSONResponse(status_code=400, content={"error": "í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."})

        safe_source = re.sub(r'[\\/]', '_', source.strip()) or "TEMP"
        txt_filename = f"{safe_source}.txt"
        txt_path = os.path.join(DOCS_DIR, txt_filename)

        logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ì—…ë¡œë“œ ìš”ì²­ - source: {safe_source}")

        # 2. ì¤‘ë³µ ì§ˆë¬¸ í•„í„°ë§
        new_questions = extract_questions(text.strip())
        existing_text = ""
        existing_questions = set()

        if os.path.exists(txt_path):
            with open(txt_path, "r", encoding="utf-8") as f:
                existing_text = f.read()
                existing_questions = extract_questions(existing_text)

        duplicated = new_questions & existing_questions
        if duplicated:
            return JSONResponse(
                status_code=200,
                content={
                    "message": "ì¼ë¶€ ë˜ëŠ” ì „ì²´ ì§ˆë¬¸ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.",
                    "duplicated_questions": list(duplicated)
                }
            )

        # 3. í…ìŠ¤íŠ¸ íŒŒì¼ì— append ì €ì¥
        with open(txt_path, "a", encoding="utf-8") as f:
            if existing_text:
                f.write("\n")
            f.write(text.strip())

        # 4. ìƒ‰ì¸ ì²˜ë¦¬ (database.pyì˜ index_document í˜¸ì¶œ)
        if index_document(txt_path, file_type="txt", force=True):
            logger.info(f"âœ… '{safe_source}' í…ìŠ¤íŠ¸ ìƒ‰ì¸ ì™„ë£Œ")
            return {
                "message": f"'{safe_source}' í…ìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€ë˜ê³  ì¬ìƒ‰ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.",
            }
        else:
            return JSONResponse(status_code=500, content={"error": "ë¬¸ì„œ ìƒ‰ì¸ ì‹¤íŒ¨"})

    except Exception as e:
        logger.error(f"âŒ upload_text ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.get("/indexed_files/")
async def get_indexed_files_endpoint():
    try:
        files = get_indexed_files()
        # Add download URLs for each file
        files_with_urls = [
            {
                "filename": filename,
                "download_url": f"/download/{filename}"
            }
            for filename in files
        ]
        return {"indexed_files": files_with_urls}
    except Exception as e:
        logger.error(f"âŒ Error getting indexed files: {str(e)}")
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.get("/download/{filename}")
async def download_file(filename: str):
    try:
        # Validate file extension
        if not filename.endswith((".pdf", ".txt")):
            return JSONResponse(
                status_code=400,
                content={"error": "Only .pdf and .txt files are supported"}
            )
            
        file_path = os.path.join(DOCS_DIR, filename)
        
        if not os.path.exists(file_path):
            return JSONResponse(
                status_code=404,
                content={"error": f"File '{filename}' not found"}
            )
            
        return FileResponse(
            path=file_path,
            filename=filename,
            media_type="application/octet-stream"
        )
            
    except Exception as e:
        logger.error(f"âŒ Error downloading file: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

@app.post("/ask/")
async def ask_question(question: str = Form(...)):
    try:
        logger.info(f"ğŸ’¬ Question received: {question}")
        result = run_qa_chain(question)

        if "error" in result:
            return JSONResponse(status_code=400, content={"error": result["error"]})
        
        sources_text = format_sources(result["sources"])

        return {
            "question": result["question"],
            "answer": result["answer"],
            # "sources": result["sources"],
            "formatted_response": f"{result['answer']}{sources_text}"
        }

    except Exception as e:
        logger.error(f"âŒ Error processing question: {str(e)}")
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post("/slack/events")
async def slack_event_listener(
    request: Request,
    background_tasks: BackgroundTasks,
    x_slack_retry_num: str = Header(default=None),
    x_slack_retry_reason: str = Header(default=None)
):
    try:
        data = await request.json()

        # ì¤‘ë³µ ì „ì†¡ ë°©ì§€
        if x_slack_retry_num:
            return {"status": "ok"}

        # Slack URL ì¸ì¦
        if data.get("type") == "url_verification":
            return {"challenge": data.get("challenge")}

        # ì‹¤ì œ ì´ë²¤íŠ¸ ì²˜ë¦¬
        if data.get("type") == "event_callback":
            event = data.get("event", {})
            event_type = event.get("type")
            user = event.get("user", "ì•Œ ìˆ˜ ì—†ìŒ")

            # ì•± ë©˜ì…˜
            if event_type == "app_mention":
                channel = event.get("channel")
                thread_ts = event.get("thread_ts", event.get("ts"))
                text = clean_mention(event.get("text", ""))
                logger.info(f"ğŸ“¥ ì±„ë„ ìˆ˜ì‹ ëœ ë©”ì‹œì§€ {user}: {text}")
                background_tasks.add_task(handle_slack_message, text, channel, thread_ts, event.get("ts"))

            # DM ë©”ì‹œì§€
            elif event_type == "message" and event.get("channel_type") == "im" and not event.get("bot_id"):
                channel = event.get("channel")
                text = event.get("text", "")
                logger.info(f"ğŸ“¥ ì•± ë©”ì„¸ì§€ íƒ­ ìˆ˜ì‹ ëœ ë©”ì‹œì§€ {user} : {text}")
                background_tasks.add_task(handle_slack_message, text, channel, None, event.get("ts"))

        return {"status": "ok"}

    except Exception as e:
        logger.error(f"âŒ Error handling Slack event: {str(e)}")
        return JSONResponse(status_code=500, content={"error": str(e)})





@app.delete("/files/{filename}")
async def delete_file(filename: str):
    try:
        # Validate file extension
        if not filename.endswith((".pdf", ".txt")):
            return JSONResponse(
                status_code=400,
                content={"error": "Only .pdf and .txt files are supported"}
            )
            
        file_path = os.path.join(DOCS_DIR, filename)
        
        if remove_document(file_path):
            return {"message": f"File '{filename}' deleted successfully"}
        else:
            return JSONResponse(
                status_code=404,
                content={"error": f"File '{filename}' not found"}
            )
            
    except Exception as e:
        logger.error(f"âŒ Error deleting file: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

@app.post("/indexed_files")
async def reindex_all_files():
    try:
        logger.info("ğŸ”„ Starting complete reindexing process...")
        
        # First, reset the Chroma DB
        if not reset_vectordb():
            return JSONResponse(
                status_code=500,
                content={"error": "Failed to reset database"}
            )
        
        logger.info("ğŸ—‘ï¸ Database reset complete, starting reindexing...")
        indexed_count = 0
        error_count = 0
        
        # Get list of all files
        for filename in os.listdir(DOCS_DIR):
            if filename.endswith((".pdf", ".txt")):
                file_path = os.path.join(DOCS_DIR, filename)
                file_type = "pdf" if filename.endswith(".pdf") else "txt"
                
                try:
                    # No need for force=True since DB is fresh
                    if index_document(file_path, file_type):
                        indexed_count += 1
                        logger.info(f"âœ… Indexed: {filename}")
                    else:
                        error_count += 1
                        logger.error(f"âŒ Failed to index: {filename}")
                except Exception as e:
                    error_count += 1
                    logger.error(f"âŒ Error indexing {filename}: {str(e)}")
        
        message = f"ì „ì²´ ì¬ìƒ‰ì¸ ì™„ë£Œ: {indexed_count}ê°œ ì„±ê³µ"
        if error_count > 0:
            message += f", {error_count}ê°œ ì‹¤íŒ¨"
            
        logger.info(message)
        return {"message": message}
        
    except Exception as e:
        error_msg = f"âŒ Error during reindexing: {str(e)}"
        logger.error(error_msg)
        return JSONResponse(status_code=500, content={"error": error_msg})

@app.post("/index_url/")
async def upload_url(url: str = Form(...), source: str = Form(default="web")):
    try:
        logger.info(f"ğŸŒ URL í¬ë¡¤ë§ ìš”ì²­: {url}")

        # âœ… ì›¹ í˜ì´ì§€ ìš”ì²­ ë° íŒŒì‹±
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            return JSONResponse(status_code=400, content={"error": f"Failed to fetch URL: {response.status_code}"})

        soup = BeautifulSoup(response.text, "html.parser")

        # âœ… ì£¼ìš” íƒœê·¸ ìœ„ì£¼ë¡œ í…ìŠ¤íŠ¸ êµ¬ì¡°í™”
        lines = []

        # ì œëª© ê³„ì—´ ë¨¼ì €
        for header in soup.find_all(['h1', 'h2', 'h3', 'h4']):
            lines.append(f"# {header.get_text(strip=True)}")

        # ë‹¨ë½
        for paragraph in soup.find_all('p'):
            lines.append(paragraph.get_text(strip=True))

        # ë¦¬ìŠ¤íŠ¸
        for li in soup.find_all('li'):
            lines.append(f"- {li.get_text(strip=True)}")

        # ê¸°íƒ€ í…ìŠ¤íŠ¸ ëˆ„ë½ ë°©ì§€ìš© (ê¸°ë³¸ì  bodyì—ì„œ ì¶”ê°€ë¡œ ê°€ì ¸ì˜¤ê¸°)
        body_text = soup.body.get_text(separator="\n", strip=True) if soup.body else ""
        lines.append(body_text)

        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        clean_lines = []
        for line in lines:
            line = line.strip()
            if line and line not in clean_lines:
                clean_lines.append(line)

        text = "\n".join(clean_lines)

        # âœ… íŒŒì¼ ì €ì¥
        safe_source = re.sub(r'[\\/]', '_', source.strip()) or "web"
        txt_filename = f"{safe_source}.txt"
        txt_path = os.path.join(DOCS_DIR, txt_filename)
        with open(txt_path, "w", encoding="utf-8") as f:
            f.write(text)

        # âœ… ìƒ‰ì¸ ì²˜ë¦¬
        if index_document(txt_path, "txt", force=True):
            return {"message": f"'{url}' í¬ë¡¤ë§ ë° ìƒ‰ì¸ ì„±ê³µ", "source": txt_filename}
        else:
            return JSONResponse(status_code=500, content={"error": "ë¬¸ì„œ ìƒ‰ì¸ ì‹¤íŒ¨"})

    except Exception as e:
        logger.error(f"âŒ upload_url ì˜¤ë¥˜: {str(e)}")
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.post("/ask_preview/", response_model=List[SearchResult])
def ask_preview(req: SearchInput):
    return get_retrieved_chunks_before_llm(
        question=req.question,
        chroma_dir=CHROMA_DIR,
        top_k=req.top_k,
        fetch_k=req.fetch_k
    )